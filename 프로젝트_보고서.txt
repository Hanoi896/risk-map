================================================================================
Global Disaster Risk Intelligence Platform (G-DRIP)
전 세계 자연재해 위험 시각화 프로젝트
================================================================================


📋 제작한 내용
================================================================================

본 프로젝트는 NASA EONET, GDACS, ReliefWeb 등 전 세계 재난 데이터 소스를 통합하여 
실시간으로 자연재해를 시각화하는 웹 기반 인텔리전스 플랫폼입니다. 엔터프라이즈급 
시스템 아키텍처를 적용하여 의사결정자에게 실행 가능한 인사이트를 제공하는 것을 
목표로 개발되었습니다.


[핵심 기능 및 시스템 구성]

• 다중 소스 데이터 통합
  - NASA EONET(지진, 화산), GDACS(글로벌 재난 경보), ReliefWeb(인도주의 위기) 등 
    이종 데이터를 실시간으로 수집하고 정규화
  - 각 데이터 소스의 상이한 JSON 구조를 표준화된 스키마로 변환
  - SQLite 데이터베이스에 통합 저장하여 단일 접점 제공

• AI 기반 위험 분석 엔진 (PRAE - Predictive Risk Analytics Engine)
  - 독자 개발한 알고리즘을 통해 재해 밀집도와 시간 감쇠 모델 적용
  - 지역별 위험 점수(0-300+ 스케일)를 자동 산출
  - 격자 기반 클러스터링(Grid-Based Clustering)으로 위험 지역 식별
  - Haversine 거리 계산을 통한 정확한 지리공간 분석
  - 재해 카테고리별 가중치 시스템 (지진: 90, 화산: 80, 산불: 70 등)
  - 시간 감쇠 모델로 최근 재해에 더 높은 우선순위 부여

• 지리정보 시각화
  - Leaflet.js 기반 대화형 지도에 재해 이벤트 표시
  - AI 분석 위험 구역을 계층별로 색상 구분 (Deep Red, Red, Orange, Gold)
  - 각 위험 구역 클릭 시 상세 정보 제공 (위험 점수, 이벤트 수, 주요 재해)
  - 반투명 원형 영역으로 영향 범위 시각화 (반경 약 500km)

• 실시간 기상 정보
  - OpenWeatherMap API 연동으로 클릭한 위치의 현재 날씨 정보 제공
  - 온도, 습도, 기압, 풍속 등 상세 기상 데이터 표시

• 모듈러 아키텍처
  - Python Flask 백엔드: RESTful API 설계로 확장 가능한 엔드포인트 제공
  - SQLite 데이터베이스: 경량화된 데이터 영속성 (PostgreSQL 마이그레이션 지원)
  - ES6+ JavaScript 프론트엔드: 모듈식 컴포넌트 설계 (mapManager, uiManager, apiHandler)
  - 독립적인 fetcher 모듈: 각 데이터 소스별 전용 수집 모듈


[기술 스택]
- 백엔드: Python 3.x + Flask Framework
- 데이터베이스: SQLite (PostgreSQL/Oracle 마이그레이션 지원)
- 프론트엔드: Vanilla JavaScript ES6+, Leaflet.js
- API 설계: RESTful API/JSON 인터페이스
- 데이터 소스: NASA EONET, GDACS, ReliefWeb, OpenWeatherMap
- 알고리즘: 격자 기반 클러스터링, 시간 감쇠 모델, Haversine 거리 계산


🔧 문제 극복 과정
================================================================================

1. 이종 데이터 소스 통합 문제

[문제]
NASA EONET, GDACS, ReliefWeb는 각각 JSON 구조, 업데이트 주기, 좌표 형식이 
상이하여 직접적인 통합이 불가능했습니다. 특히 EONET은 이벤트 중심 구조, 
GDACS는 경보 중심 구조, ReliefWeb은 인도주의 영향 중심 구조로 설계되어 
데이터 필드명과 계층 구조가 완전히 달랐습니다.

[해결]
각 소스별 전용 fetcher 모듈(eonet_fetcher.py, gdacs_fetcher.py, disease_fetcher.py)을 
개발하여 ETL(Extract, Transform, Load) 파이프라인을 구축했습니다.
- Extract: 각 API에서 원시 JSON 데이터 수집
- Transform: 표준화된 스키마로 변환 (통일된 필드명, 날짜 형식, 좌표 체계)
- Load: 단일 SQLite 데이터베이스에 저장
이를 통해 이종 데이터를 하나의 통합된 뷰로 제공할 수 있게 되었습니다.


2. AI 위험 분석 알고리즘 설계

[문제]
단순 재해 마커 표시를 넘어 '어느 지역이 위험한가'를 정량화하는 알고리즘이 
필요했으나, 기존 오픈소스에는 이러한 기능이 없었습니다. 또한 재해의 종류, 
발생 시점, 지리적 근접성을 모두 고려하는 복합적인 분석이 요구되었습니다.

[해결]
격자 기반 클러스터링(Grid-Based Clustering)과 시간 감쇠 모델을 결합한 
독자적인 위험 스코어링 시스템을 개발했습니다.

[알고리즘 구조]
1) 이벤트 정규화: 재해 카테고리별 가중치 할당
   - 지진: 90, 화산: 80, 산불: 70, 가뭄: 65, 홍수: 60 등
   
2) 시간 감쇠 모델: 최신성 반영
   - 3일 이내: +20점
   - 7일 이내: +10점
   - 30일 이내: +5점
   - 30일 이상: +0점

3) 공간 집계: 격자 기반 클러스터링
   - 5도 단위 격자로 전 세계 분할
   - 같은 격자 내 이벤트 점수 합산
   - 무게중심 좌표 계산으로 대표 위치 산출

4) 위험 등급 분류
   - Deep Red (300+): 극도로 위험
   - Red (150+): 높은 위험
   - Orange (80+): 주의 필요
   - Gold (30+): 경계 상태

이 알고리즘은 해석 가능한 규칙 기반 시스템으로, 블랙박스 ML과 달리 
모든 계산 과정을 추적하고 검증할 수 있습니다.


3. 프론트엔드-백엔드 비동기 통신 최적화

[문제]
500개 이상의 재해 데이터와 AI 분석 결과를 동시에 로드할 때 초기 로딩 시간이 
3초 이상 소요되어 사용자 경험이 저하되었습니다. 특히 AI 위험 분석 계산이 
서버 측에서 1-2초가량 걸려 병목 현상이 발생했습니다.

[해결]
다음과 같은 최적화를 적용했습니다:

1) API 엔드포인트 분리
   - /api/disasters: 재해 이벤트 데이터만 제공
   - /api/risk-analysis: AI 분석 결과만 제공
   - 독립적인 로딩으로 사용자가 먼저 지도를 볼 수 있게 함

2) 병렬 요청 처리
   - 프론트엔드에서 Promise.all()을 사용한 동시 API 호출
   - 두 요청이 병렬로 처리되어 대기 시간 단축

3) 데이터베이스 쿼리 최적화
   - 최신 500건으로 제한 (ORDER BY date DESC LIMIT 500)
   - 인덱스 추가로 쿼리 성능 향상

4) 클라이언트 측 캐싱
   - 한 번 로드한 데이터는 메모리에 보관
   - 토글 시 재요청 없이 즉시 표시

결과적으로 로딩 시간을 1초 이내로 단축했습니다.


4. 한글 문서화 및 전문성 강화

[문제]
초기 README는 기능 나열 수준이었으나, 프로젝트의 기술적 깊이를 충분히 
전달하지 못했습니다. 특히 AI 위험 분석 알고리즘의 동작 원리가 설명되지 
않아 단순한 시각화 도구로 오해받을 가능성이 있었습니다.

[해결]
엔터프라이즈급 문서 구조를 채택하여 다음 섹션을 추가했습니다:

1) 경영 요약: 프로젝트의 비즈니스 가치 명확화
2) 시스템 아키텍처: 백엔드/프론트엔드 구조 상세 설명
3) 알고리즘 방법론: 수학 공식과 Python 코드 예시 포함
   - Haversine 거리 계산 공식
   - 격자 클러스터링 알고리즘 구현
   - 시간 감쇠 모델 수식
   - 위험 등급 분류 기준
4) 데이터 소스 및 API 문서화
5) 향후 개선 방향 (DBSCAN 클러스터링, 머신러닝 최적화 등)

결과적으로 기술적 깊이와 전문성을 보여주는 포트폴리오급 문서가 완성되었습니다.


💭 소감
================================================================================

이번 프로젝트를 통해 단순한 데이터 시각화를 넘어 실제 사회적 가치를 제공할 수 있는 
시스템을 설계하는 경험을 할 수 있었습니다. 특히 세 가지 측면에서 큰 성장을 느꼈습니다.


[시스템 통합 역량]
여러 외부 API를 통합하면서 데이터 정규화, 에러 핸들링, 스케줄링 등 
실무 엔지니어링 스킬을 체득했습니다. 특히 각 API의 rate limit과 응답 형식 차이를 
고려한 설계의 중요성을 깨달았습니다. 

예를 들어 NASA EONET은 무제한 요청이 가능하지만, OpenWeatherMap은 분당 60회 
제한이 있어 캐싱 전략이 필수였습니다. 또한 GDACS는 XML 응답을 제공하여 
JSON 파싱과 다른 접근법이 필요했습니다. 이러한 실무적인 제약 조건들을 
하나씩 해결해가며 시스템 설계 능력이 크게 향상되었습니다.


[알고리즘 설계 능력]
Haversine 거리 계산, 격자 기반 클러스터링, 시간 감쇠 모델 등 지리공간 분석 
알고리즘을 직접 구현하며 이론과 실무의 간극을 메우는 법을 배웠습니다. 

특히 '해석 가능한 AI'의 가치를 실감했습니다. 처음에는 머신러닝 기반 예측 모델을 
고려했으나, 학습 데이터 부족과 결과 해석의 어려움으로 규칙 기반 시스템을 선택했습니다. 
이 결정이 오히려 프로젝트의 강점이 되었습니다. 모든 위험 점수의 계산 과정을 
추적할 수 있어 신뢰성이 높고, 파라미터 튜닝도 직관적이었습니다.

또한 격자 크기를 5도로 설정한 이유, 카테고리별 가중치를 어떻게 결정했는지 등 
모든 설계 결정에 명확한 근거를 제시할 수 있게 되었습니다.


[문서화의 중요성]
코드만큼이나 문서화가 프로젝트의 완성도를 좌우한다는 것을 배웠습니다. 
기술적 깊이를 전달하면서도 비전문가도 이해할 수 있는 균형 잡힌 문서 작성이 
얼마나 어려운지 알게 되었습니다.

README를 3번 전면 개편하면서 깨달은 점은, 좋은 문서는 '무엇을 만들었는가'뿐만 아니라 
'왜 이렇게 만들었는가'를 설명해야 한다는 것입니다. 경영 요약, 알고리즘 해부, 
향후 개선 방향 등을 추가하면서 프로젝트의 전문성이 크게 향상되었습니다.


[향후 발전 방향]
이 프로젝트를 발판으로 다음과 같은 확장을 계획하고 있습니다:

1) 머신러닝 기반 재해 예측
   - 과거 재해 패턴 학습
   - 계절성 및 지역별 특성 반영
   - 3일/7일/30일 예측 모델 개발

2) 인구 밀도 가중치 통합
   - UN 인구 데이터 연동
   - 예상 피해 인원 추정
   - 우선 대응 지역 식별

3) 실시간 알림 시스템
   - 웹소켓 기반 푸시 알림
   - 이메일/SMS 경보 발송
   - 사용자 맞춤형 위험 지역 설정

4) 모바일 앱 개발
   - React Native 기반 크로스 플랫폼 앱
   - 오프라인 모드 지원
   - GPS 기반 현재 위치 위험도 알림

이번 프로젝트는 단순한 과제를 넘어 제 커리어의 포트폴리오 핵심 프로젝트가 
될 것입니다. 실제 재난 대응 기관이나 NGO에서 활용 가능한 수준까지 발전시켜 
사회적 가치를 창출하고 싶습니다.


================================================================================
작성일: 2025년 12월 30일
프로젝트: Global Disaster Risk Intelligence Platform (G-DRIP)
================================================================================
